---
title: 'UCLA Biostatistics 285: Homework 2'
subtitle: 'Instructor: Dr. Michele Guindani'
author: "Soumyakanti Pan, e-mail: span18@ucla.edu"
date: \today
output: 
  pdf_document:
    toc: false
    number_sections: true
    df_print: kable
header-includes:
  - \usepackage[utf8]{inputenc}
  - \usepackage[T1]{fontenc}
  - \usepackage{amsmath, amssymb, amsfonts, bm}
  - \newcommand{\given}{\,|\,}
  - \usepackage[ruled,vlined,linesnumbered]{algorithm2e}
  - \newcommand{\mb}[1]{\mathbf{#1}}
  - \DeclareUnicodeCharacter{03B1}{$\alpha$}
bibliography: refs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, fig.showtext = TRUE,
                      opts.label="kill_prefix", messages = FALSE)
```

```{r, echo=FALSE}
list.of.packages <- c("lattice", "latticeExtra", "showtext", "ggplot2",
                      "gridExtra", "readr", "BNPmix")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
```

\section{Problem 1}

\subsection{Part 1}

The function `Sethu_jump` generates the jumps given a truncation option and a $\alpha$. The `generate_DPH` uses a jump function (here we use the `Sethu_jump`) and takes input a base measure and its parameters along with $\alpha$, truncation parameter $K$ and number of samples to be generated. The final output is realizations of $\text{DP}(\alpha, \mathcal{N}(0, 1))$ approximated by finite truncation with 20 terms as described in @ishwaranzarepour.

```{r, echo=FALSE, message=FALSE}
library(lattice)
library(latticeExtra)
library(showtext)
```

```{r, echo = FALSE}
K.trunc = 20
Sethu_jump = function(alpha, K = K.trunc){
  v = w = array(dim = K)
  v[K] = 1
  v[1:(K - 1)] = rbeta(n = K - 1, 1, alpha)
  w[1] = v[1]
  for(h in 2:K){
    w[h] = v[h] * prod(1 - v[1:(h - 1)])
  }
  return(w)
}

generate_DPH = function(alpha, K, N.sample, 
                        base, baseparams, 
                        jump = Sethu_jump){
  if(base == 'norm' && length(baseparams) == 2){
    m = baseparams[1]
    s = baseparams[2]
    atoms = rnorm(n = K, mean = m, sd = s)
  }else stop("Incorrect base measure/parameters.")
  w = jump(alpha, K)
  dph.sample = sample(x = atoms, replace = T, 
                      size = N.sample, prob = w)
  return(dph.sample)
}
```

```{r, echo=FALSE, fig.height=3.5, fig.width=7, fig.cap="Prior c.d.f realizations of Dirichlet process with different base measures."}
set.seed(1729)
N.S = 50
sample.size = 100
alphaseq = c(0.1, 0.5, 1, 10)
dph.samples = NULL
for(alpha in alphaseq){
  dph.samples.temp = array(dim = c(sample.size, N.S))
  for(i in 1:N.S){
    dph.samples.temp[, i] = generate_DPH(alpha = alpha, K = 20, 
                                         N.sample = sample.size,
                                         base = "norm", baseparams = c(0, 1))
  }
  xnam = paste("x", 1:N.S, sep = "")
  colnames(dph.samples.temp) = xnam
  dph.samples.temp = as.data.frame(dph.samples.temp)
  dph.samples = rbind(dph.samples, dph.samples.temp)
}
alpha.labels = paste(expression(α), "=",
                     as.character(alphaseq), sep = " ")
dph.samples$alpha = rep(alpha.labels, each = sample.size)

fmla1 = as.formula(paste("~ ", paste(xnam, collapse= "+"), " | alpha"))
ecdfplot(fmla1, data = dph.samples, lwd = 0.5,
         xlab = "", ylab = "", xaxt = "n", asp = 1.5)
```

We can also get Monte Carlo estimates of the mean functional $\mu(G)$ and the variance functional $\sigma^2(G)$ from the prior realizations of $G$, drawn by assuming a truncation upto $K = 20$ terms. 

```{r, echo=FALSE, message=FALSE}
mv_functional = function(n.G, alpha, 
                         sample.size = 100,
                         K = 20, base = "norm",
                         baseparams = c(0, 1)){
  dph.samples.temp = array(dim = c(sample.size, n.G))
  for(i in 1:n.G){
    dph.samples.temp[, i] = generate_DPH(alpha = alpha, K = K, 
                                         N.sample = sample.size,
                                         base = base, 
                                         baseparams = baseparams)
  }
  m.G = apply(dph.samples.temp, 2, mean)   # mean functional
  v.G = apply(dph.samples.temp, 2, var)     # variance functional
  return(cbind(m.G, v.G))
}
```
```{r, echo = FALSE}
n.G = 1000
mv.df = data.frame(mean = as.numeric(),
                   var = as.numeric())
for(alpha in alphaseq){
  temp = mv_functional(n.G = n.G, alpha = alpha)
  mv.df = rbind(mv.df, temp)
}

names(mv.df) = c("mean", "var")
mv.df$alpha = rep(alpha.labels, each = n.G)
```
```{r, message=FALSE, echo=FALSE, fig.height=3, fig.width=7, fig.align='center', fig.cap="Prior distributions of mean and variance functionals from 1000 prior samples"}
library(ggplot2)

plot.mu = ggplot(mv.df) +
  geom_histogram(aes(x = mean, y = after_stat(density)), 
                 binwidth = 0.1) +
  geom_density(aes(x = mean), lwd = 0.25, colour = 4,
               fill = 4, alpha = 0.25) +
  facet_wrap(~ alpha, ncol = 2) +
  xlab(expression(paste(mu, (G)))) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank())

plot.var = ggplot(mv.df) +
  geom_histogram(aes(x = var, y = after_stat(density)), 
                 bins = 30, colour = "darkred",
               fill = "darkred") +
  facet_wrap(~ alpha, ncol = 2) +
  xlab(expression(paste(sigma^2, (G)))) +
  theme_bw() +
  theme(panel.grid.minor = element_blank(),
        panel.grid.major = element_blank())

gridExtra::grid.arrange(plot.mu, plot.var, ncol = 2)
```
We see in Figure 2, that naturally the mean functional $\mu(G)$ is centered around 0 since the base measure is centered around 0 but higher the value of $\alpha$, more is the concentration of the mean functional around 0. In other words, the mean functional has higher kurtosis for higher $\alpha$. On the other hand, the variance functional indicates that lower value of $\alpha$ indicates the lower dispersion of atoms in $G$, whereas, higher value of $\alpha$ indicates higher dispersion of atoms in $G$.

\subsection{Part 2}

Following Weak Law of large Numbers (WLLN), we can estimate the expected number of nonempty clusters $E(M)$ by the mean number of unique atoms from each realization of c.d.f. sampled from the prior $\text{DP}(\alpha, \mathcal{N}(0, 1))$ with different values of $\alpha$. Here, we have considered $\alpha = 0.1, 0.5, 1, 10$. Note that the sampled prior realizations are generated from a approximate prior achieved by truncation the infinite mixture to a mixture of 20 atoms. The red line in Figure 3 denotes the theoretical expected number of nonempty clusters as shown by @antoniak74 given by $E(M) = \alpha \log ((\alpha + n) / \alpha)$. We see for smaller values of $\alpha$, the approximation may have given reasonable estimates of number of non-empty clusters but I suspect that for a large value of $\alpha$, the truncation approximation might not be reasonable.

```{r, echo=FALSE}
set.seed(1729)
N.S = 1000
sample.size = 100
alphaseq = c(0.1, 0.5, 1, 10)
E.M.dph = array(dim = c(N.S, length(alphaseq)))
for(j in 1:length(alphaseq)){
  alpha = alphaseq[j]
  dph.samples.temp = array(dim = c(sample.size, N.S))
  for(i in 1:N.S){
    dph.samples.temp[, i] = generate_DPH(alpha = alpha, K = 20, 
                                         N.sample = sample.size,
                                         base = "norm", baseparams = c(0, 1))
  }
  m = apply(dph.samples.temp, 2, function(x) length(unique(x)))
  E.M.dph[, j] = m
}
E.M.theo = alphaseq * log((alphaseq + sample.size) / alphaseq)
alpha.grid = c(seq(0, 0.5, length.out = 20),
               seq(0.5, 1, length.out = 20),
               seq(1, 10, length.out = 20))
E.M.theo = alpha.grid * log((alpha.grid + sample.size) / alpha.grid)
```

```{r, echo=FALSE, fig.align='center', fig.height=4, fig.width=5, fig.cap="Comparison of theoretical expected number of non-empty clusters and estimated with finite truncation approximation based on 1000 samples"}
plot(alpha.grid, E.M.theo, col = "red", xlim = c(0, 10.5),
     xlab = expression(paste(alpha)), ylab = expression(paste(E(M))), type = "l")
boxplot(E.M.dph, boxwex = c(0.25, 0.25, 0.25, 0.75), xaxt = "n", 
        whisklty = 1, whisklwd = 0.5, 
        staplelty = 1, staplelwd = 0.5,
        outpch = ".", outcol = "grey", outcex = 0.5, 
        medcol = "darkblue", col = "lightblue",
        at = alphaseq, add = T)
```

\subsection{Part 3}

As we know that, we can construct a Dirichlet process on $(\mathfrak{X}, \mathfrak{S}, P)$ from a Pólya sequence, invoking de Finetti's theorem, assuming $i < j$, we can write the distribution of the $j$-th observation as
\[
X_j \mid X_{j-1}, \dots, X_i, \dots, X_1 \sim \frac{\alpha}{\alpha + j - 1} G_0 + \frac{1}{\alpha + j - 1} \sum_{k = 1}^{j-1} \delta_{X_k}. 
\]
Hence, for any event $A \in \mathfrak{S}$, we can calculate the probability $P(X_j \in A)$ as follows
\[
P(X_j \in A) = \frac{\alpha}{\alpha + j - 1} G_0 (A) + \frac{1}{\alpha + j - 1} \sum_{k = 1}^{j-1} \delta_{X_k}(A).
\]
Considering $A = \mathbf{1}(X_i)$ denoting the event that the observed value is equal to $X_i$, we can calculate the above quantity following from $G_0(A) = 0$ as $G_0$ is non-atomic and 
\[
P(X_j = X_i) =  \frac{1}{\alpha + j - 1} \sum_{k = 1}^{j-1} \delta_{X_k}(A) = \frac{n_i}{\alpha + j - 1}
\]
where $n_i$ is the number of occurrence of $X_i$ among the samples $X_1, X_2, \dots, X_{j-1}$.

\subsection{Part 4}

We know from the conjugacy of a Dirichlet process, if we have the data generation model as $X_1, X_2, \dots, X_n \overset{\text{ind}}{\sim} G$ with $G \sim \text{DP}(\alpha G_0), G_0 = \mathcal{N}(0, 1)$, then we can write the posterior as again a Dirichket process with a different base measure.
\[
G \mid X_1, X_2, \dots, X_n \sim \text{DP}(\alpha + n, \frac{1}{\alpha + n} \sum_{i = 1}^n \delta_{X_i} + \frac{\alpha}{\alpha + n} G_0)
\]
Hence, from Figure 4, we see that with increase in $\alpha$, we see more posterior samples (in gray) tending towards the prior base measure (given in red dashed line) and for lesser values of $\alpha$, we have more attenuation towards the empirical cdf of the given data (drawn in solid black line).
```{r posterior, message=FALSE, echo=FALSE}
library(readr)
DPdata <- read_csv("DPdata.csv", col_types = cols(...1 = col_skip()))

posterior_DPH = function(alpha, K = 50, 
                         N.sample = 1000, n.postG,
                         jump = Sethu_jump,
                         dat){
  n = length(dat)
  post.samples = array(dim = c(N.sample, n.postG))
  for(j in 1:n.postG){
    temp = runif(K)
    atoms = array(dim = K)
    for(i in 1:K){
      if(temp[i] < alpha/(alpha + n))  atoms[i] = rnorm(1, 0, 1)
      else atoms[i] = sample(dat, size = 1)
    }
    w = jump(alpha + n, K)
    post.samples[, j] = sample(x = atoms, replace = T, 
                               size = N.sample, prob = w)
  }
  xnam = paste("pG", 1:n.postG, sep = "")
  colnames(post.samples) = xnam
  post.samples = as.data.frame(post.samples)
  return(post.samples)
}

set.seed(1729)
N.postG = 50
sample.size = 1000
alphaseq = c(0.1, 0.5, 1, 10)
post.samples = NULL
for(alpha in alphaseq){
  post.samples.temp = array(dim = c(sample.size, N.postG))
  for(i in 1:N.postG){
    post.samples.temp = posterior_DPH(alpha = alpha, K = 50, 
                                      N.sample = sample.size,
                                      n.postG = N.postG,
                                      jump = Sethu_jump,
                                      dat = as.numeric(DPdata$x))
  }
  xnam = paste("pG", 1:N.postG, sep = "")
  colnames(post.samples.temp) = xnam
  post.samples.temp = as.data.frame(post.samples.temp)
  post.samples.temp$x = as.numeric(DPdata$x)
  post.samples = rbind(post.samples, post.samples.temp)
}
alpha.labels = paste("\U03B1 =",
                     as.character(alphaseq), sep = " ")
post.samples$alpha = rep(alpha.labels, each = sample.size)
```
```{r, echo=FALSE, fig.height=3.5, fig.width=7, fig.align='center', fig.cap="Posterior samples (gray) of the generative random measure compared with the empirical c.d.f. (black) and the original base measure of the DP prior (red)."}
fmla2 = as.formula(paste("~ ", paste(c(xnam, "x"), collapse= "+"), " | alpha"))
normseq = seq(min(post.samples[,1:N.postG]), 
              max(post.samples[,1:N.postG]), length.out = 1000)
ecdfplot(fmla2, data = post.samples,
         col = rep(c("gray", "black"), times = c(N.postG, 1)),
         xlab = "", ylab = "", xaxt = "n", asp = 1.5) +
  as.layer(xyplot(pnorm(normseq) ~ normseq, type = "l", 
                  col = "red", lty = 2, lwd = 2))
```

\subsection{Part 5}

Now if the prior base measure is atomic, $G_0 = \text{Poisson}(3)$, $G_0(A)$ will no longer be 0 and hence we will have
\[
P(X_j = X_i) =  G_0(A) + \frac{n_i}{\alpha + j - 1} = \frac{e^{-3}3^{X_i}}{\Gamma (X_i+1)} + \frac{n_i}{\alpha + j - 1}
\]
where $n_i$ is the number of occurrence of $X_i$ among the samples $X_1, X_2, \dots, X_{j-1}$.

\section{Problem 2}

```{r bnpmix, echo=FALSE, results='hide'}
library(BNPmix)
ffdat = cbind(faithful$eruptions, faithful$waiting)

DPprior <- PYcalibrate(Ek = 2, n = nrow(ffdat), discount = 0)
prior <- list(strength = DPprior$strength, discount = 0)
# prior <- list(strength = 1, discount = 0)
min1 = 0.8*min(ffdat[,1]); max1 = 1.2*max(ffdat[,1])
min2 = 0.8*min(ffdat[,2]); max2 = 1.2*max(ffdat[,2])
grid <- expand.grid(seq(min1, max1, length.out = 100),
                    seq(min2, max2, length.out = 100))
output = list(grid = grid, out_type = "FULL")
mcmc <- list(niter = 5000, nburn = 1000, m_imp = 100)
set.seed(42)
fit2 <- PYdensity(y = ffdat, mcmc = mcmc, prior = prior, output = output)

prior <- list(strength = 1, discount = 0)
min1 = 0.8*min(ffdat[,1]); max1 = 1.2*max(ffdat[,1])
min2 = 0.8*min(ffdat[,2]); max2 = 1.2*max(ffdat[,2])
grid <- expand.grid(seq(min1, max1, length.out = 100),
                    seq(min2, max2, length.out = 100))
output = list(grid = grid, out_type = "FULL")
mcmc <- list(niter = 5000, nburn = 1000, m_imp = 100)
set.seed(42)
fit3 <- PYdensity(y = ffdat, mcmc = mcmc, prior = prior, output = output)
```
```{r, echo=FALSE, fig.align='center', fig.height=3, fig.width=3.5, fig.cap="Posterior density"}
plot(fit2, dim = c(1,2), show_clust =  TRUE,
     xlab="eruptions", ylab="waiting")
```
```{r, echo=FALSE, fig.height=3, fig.width=7, fig.align='center', fig.cap="Posterior samples of number of clusters with different values of strength in PY prior"}
par(mfrow = c(1, 2))
barplot(table(apply(fit2$clust, 1, max)), col = "midnightblue", yaxt = "n")
barplot(table(apply(fit3$clust, 1, max)), col = "midnightblue", yaxt = "n")
```




\section*{References}

<div id="refs"></div>